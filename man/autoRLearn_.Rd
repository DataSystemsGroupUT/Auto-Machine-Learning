% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autoRLearn_.R
\name{autoRLearn_}
\alias{autoRLearn_}
\title{Advanced version of autoRLearn.}
\usage{
autoRLearn_(
  df_train,
  df_test,
  maxTime = 10,
  models = c("randomForest", "naiveBayes", "boosting", "l2-linear-classifier", "svm"),
  optimizationAlgorithm = "hyperband",
  bw = 3,
  max_iter = 81,
  kde_type = "single"
)
}
\arguments{
\item{df_train}{Dataframe of the training dataset. Assumes it is in perfect shape with all numeric variables and factor response variable named "class".}

\item{df_test}{Dataframe of the test dataset. Assumes it is in perfect shape with all numeric variables and factor response variable named "class".}

\item{maxTime}{Float representing the maximum time the algorithm should be run (seconds).}

\item{models}{List of strings denoting which algorithms to use for the process:
\itemize{
\item "randomForest" - Random forests using the randomForest package
\item "ranger - Random forests using the ranger package (unstable)
\item "naiveBayes" - Naive bayes using the fastNaiveBayes package
\item "boosting" - Gradient boosting using xgboost
\item "l2-linear-classifier" - Linear primal Support vector machine from LibLinear
\item "svm" - RBF kernel svm from e1071
}}

\item{optimizationAlgorithm}{- String of which hyperparameter tuning algorithm to use:
\itemize{
\item "hyperband" - Hyperband with uniformly initiated parameters
\item "bohb" - Hyperband with bayesian optimization as described on F. Hutter et al 2018 paper BOHB. Has extra parameters bw and kde_type
}}

\item{bw}{- (only applies to BOHB) Double representing how much should the KDE bandwidth be widened. Higher values allow the algorithm to explore more hyperparameter combinations}

\item{max_iter}{- (affects both hyperband and BOHB) Integer representing the maximum number of iterations that one successive halving run can have}

\item{kde_type}{- (only applies to BOHB) String representing whether a model's hyperparameters should be tuned individually of each other or have their probability densities multiplied:
\itemize{
\item "single" - each hyperparameter has its own expected improvement calculated
\item "mixed" - all hyperparameters' probabilty densities are multiplied and only one mixed expected improvement is calculated
}}
}
\value{
List of Results
\itemize{
\item \code{perf} - accuracy of the best performing model on the test data
\item \code{pred} - prediction on the test data using the best model
\item \code{model} - best model object
\item \code{best_models} - table with the best hyperparameters found for the selected models.
}
}
\description{
Tunes the hyperparameters of the desired algorithm/s using either hyperband or BOHB.
}
